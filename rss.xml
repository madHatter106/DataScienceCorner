<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Erdem Karaköylü 's DS corner</title><link>https://madhatter106.github.io/</link><description>On confronting data...</description><atom:link href="https://madhatter106.github.io/DataScienceCorner/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Tue, 16 Jul 2019 14:53:03 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Correlation &amp; Causation - Part 1</title><link>https://madhatter106.github.io/DataScienceCorner/posts/correlation-causation-part-1/</link><dc:creator>Erdem Karaköylü</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;"Correlation is not causation" goes the saying, cautioning against spurious correlation. Causally related phenomena, however, do correlate. How then can one differentiate spurious correlation from causal correlation, when confronted with a modeling problem? The following recommendations and simple simulation like the one making the subject of this post go a long way.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;use causal DAGs - Directed Acyclic Graphs - to represent the current understanding of how the various pieces of data might be causally influencing each other (more on DAGs below.)&lt;/li&gt;
&lt;li&gt;be aware of at least the four basic confounding mechanisms that lead to spurious correlation; namely, the Collider, the Pipe, the Fork, the Descendent.&lt;/li&gt;
&lt;li&gt;be aware that spurious correlation through confounding can lead to &lt;u&gt;better&lt;/u&gt; model performance that are ultimately misguiding and that can lead further inquiry astray.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://madhatter106.github.io/DataScienceCorner/posts/correlation-causation-part-1/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://madhatter106.github.io/DataScienceCorner/posts/correlation-causation-part-1/</guid><pubDate>Mon, 15 Jul 2019 20:46:41 GMT</pubDate></item><item><title>Automatically Checking Changes in my Citizenship ApplicationStatus</title><link>https://madhatter106.github.io/DataScienceCorner/posts/automatically-checking-changes-in-my-citizenship-application-status/</link><dc:creator>Erdem Karaköylü</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I am currently in the process of acquiring my US citizenship. It's a long process without a set schedule. I found myself frequently visiting the USCIS status page to see if any update had been posted. After a few iterations I grew tired and rather than waste time going to the website, enter my case number, navigate to the status page, and check if there were any changes, I decided to automate these steps with a python script.&lt;/p&gt;
&lt;p&gt;In this post I detail the steps of the script, which does the status check for me and shoots me an e-mail when detecting a change. I then show how I made it into a cron job so that it would run automatically on a schedule of my choosing.&lt;/p&gt;
&lt;p&gt;Here I leverage &lt;a href="http://www.seleniumhq.org/projects/webdriver/"&gt;SELENIUM&lt;/a&gt;, &lt;a href="https://www.crummy.com/software/BeautifulSoup/"&gt;BEAUTIFULSOUP&lt;/a&gt;, and &lt;a href="https://docs.python.org/3/library/smtplib.html"&gt;SMTPLIB&lt;/a&gt; and &lt;a href="https://pypi.python.org/pypi/python-crontab"&gt;PYTHON-CRONTAB&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that building my automaton requires scouting the pages it needs to navigate to identify the specific elements it needs to interact with; in this case only a handful.
&lt;/p&gt;&lt;p&gt;&lt;a href="https://madhatter106.github.io/DataScienceCorner/posts/automatically-checking-changes-in-my-citizenship-application-status/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://madhatter106.github.io/DataScienceCorner/posts/automatically-checking-changes-in-my-citizenship-application-status/</guid><pubDate>Wed, 13 Dec 2017 20:40:58 GMT</pubDate></item><item><title>Relating Principle Components To The Data Whence They Came</title><link>https://madhatter106.github.io/DataScienceCorner/posts/relating-principle-components-to-the-data-whence-they-came/</link><dc:creator>Erdem Karaköylü</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;When preparing a dataset for machine learning, a common step is to reduce dimensions using &lt;a href="https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/"&gt;principal component analysis (PCA)&lt;/a&gt;. This is often necessary when multi-collinearity is an issue. Multi-collinearity is often encountered in multi- and hyper-spectral satellite data, where individual channels do not add much information to that carried by their neighbors.&lt;/p&gt;
&lt;p&gt;Here I use data from the &lt;a href="http://hico.coas.oregonstate.edu/"&gt;Hyperspectral Imager for the Coastal Ocean (HICO)&lt;/a&gt;, with several thousand observations.
&lt;/p&gt;&lt;p&gt;&lt;a href="https://madhatter106.github.io/DataScienceCorner/posts/relating-principle-components-to-the-data-whence-they-came/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://madhatter106.github.io/DataScienceCorner/posts/relating-principle-components-to-the-data-whence-they-came/</guid><pubDate>Mon, 23 Oct 2017 18:54:21 GMT</pubDate></item><item><title>Faster comparison of two Numpy arrays</title><link>https://madhatter106.github.io/DataScienceCorner/posts/speeding-up-numpy-array-comparison/</link><dc:creator>Erdem Karaköylü</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This short post details an easy way to speed up array comparison. My problem was that I had to compare two 3D arrays, A and B, These arrays are different only in their first dimension, and have equal $2^{nd}$ and $3^{rd}$ dimensions. I needed to find out whether any of the 2D arrays contained along the first dimension of, say, A were also present along the first dimension of B. The first approach one might try is based on nested for-loops. This becomes quickly unwieldy with even moderate-sized arrays. The faster alternative is to hash the 2D data in one of the arrays and store the hash table; I prefer to do that with the smaller array for space use efficiency. The next step is to go along the first dimension of the other array, hash the data and compare. This ends up requiring only serial for-loops. Note here that the Python version is 3.6, which implements numpy ndarray hashing differently than 2.x. Note also the use of &lt;a href="https://cito.github.io/blog/f-strings/"&gt;f-strings; a nifty new feature of python 3.6.&lt;/a&gt;
&lt;/p&gt;&lt;p&gt;&lt;a href="https://madhatter106.github.io/DataScienceCorner/posts/speeding-up-numpy-array-comparison/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>array hashing</category><category>Numpy</category><guid>https://madhatter106.github.io/DataScienceCorner/posts/speeding-up-numpy-array-comparison/</guid><pubDate>Mon, 25 Sep 2017 14:52:36 GMT</pubDate></item><item><title>Bayesian Approach to Chlorophyll Estimation from Satellite Remote Sensing</title><link>https://madhatter106.github.io/DataScienceCorner/posts/bayesian-approach-to-chlorophyll-estimation-from-satellite-remote-sensing/</link><dc:creator>Erdem Karaköylü</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This post describes the use of bayesian polynomial regression to estimate chlorophyll from remote sensing reflectance; the output is contrasted to that obtained via the frequentist ordinary least squares regression.&lt;/p&gt;
&lt;p&gt;Chlorophyll is estimated from ocean color remote sensing data  using one of two main types of algorithms; semi-analytical or empirical. The latter is a polynomial model, where the input is a ratio of bands and the coefficients are obtained via ordinary least squares fitting. These models are usually more successful than their semi-analytical counterparts and as a result are at the forefront of the operational algorithmic arsenal used by the Ocean Biology Processing Group at NASA Goddard. &lt;/p&gt;&lt;p&gt;&lt;a href="https://madhatter106.github.io/DataScienceCorner/posts/bayesian-approach-to-chlorophyll-estimation-from-satellite-remote-sensing/"&gt;Read more…&lt;/a&gt; (16 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bayesian regression</category><category>chlorophyll</category><category>ocean color</category><category>pymc3</category><guid>https://madhatter106.github.io/DataScienceCorner/posts/bayesian-approach-to-chlorophyll-estimation-from-satellite-remote-sensing/</guid><pubDate>Mon, 10 Apr 2017 17:41:55 GMT</pubDate></item><item><title>Getting the NASA bio-Optical Marine Algorithm Dataset (NOMAD) into a Pandas DataFrame</title><link>https://madhatter106.github.io/DataScienceCorner/posts/getting-nomadata-into-a-pandas-dataframe/</link><dc:creator>Erdem Karaköylü</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this post, I'm going to briefly describe how a I download the &lt;a href="https://seabass.gsfc.nasa.gov/wiki/NOMAD"&gt;NASA bio-Optical Marine Algorithm Dataset or NOMAD&lt;/a&gt; created for algorithm development, extract the data I need and store it all neatly in a Pandas DataFrame. Here I use the latest dataset, NOMAD v.2, created in 2008.
&lt;/p&gt;&lt;p&gt;&lt;a href="https://madhatter106.github.io/DataScienceCorner/posts/getting-nomadata-into-a-pandas-dataframe/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>chlorophyll</category><category>ocean color</category><category>pandas</category><guid>https://madhatter106.github.io/DataScienceCorner/posts/getting-nomadata-into-a-pandas-dataframe/</guid><pubDate>Wed, 15 Mar 2017 18:10:25 GMT</pubDate></item><item><title>XARRAY &amp; GEOVIEWS A new perspective on oceanographic data - part II</title><link>https://madhatter106.github.io/DataScienceCorner/posts/xarray-geoviews-a-new-perspective-on-oceanographic-data-part-ii/</link><dc:creator>Erdem Karaköylü</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In a &lt;a href="https://madhatter106.github.io/DataScienceCorner/posts/xarray-geoviews-a-new-perspective-on-oceanographic-data-part-i/"&gt;previous post&lt;/a&gt;, I introduced xarray with some simple manipulation and data plotting. In this super-short post, I'm going to do some more manipulation, using multiple input files to create a new dimension, reorganize the data and store them in multiple output files. All but with a few lines of code.
&lt;/p&gt;&lt;p&gt;&lt;a href="https://madhatter106.github.io/DataScienceCorner/posts/xarray-geoviews-a-new-perspective-on-oceanographic-data-part-ii/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>jupyter</category><category>ocean color</category><category>xarray</category><guid>https://madhatter106.github.io/DataScienceCorner/posts/xarray-geoviews-a-new-perspective-on-oceanographic-data-part-ii/</guid><pubDate>Mon, 20 Feb 2017 19:30:43 GMT</pubDate></item><item><title>XARRAY &amp; GEOVIEWS A new perspective on oceanographic data - part I</title><link>https://madhatter106.github.io/DataScienceCorner/posts/xarray-geoviews-a-new-perspective-on-oceanographic-data-part-i/</link><dc:creator>Erdem Karaköylü</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;With this post I explore an alternative to ol' numpy; &lt;a href="http://xarray.pydata.org/en/stable/index.html"&gt;xarray&lt;/a&gt;. Numpy is still running under the hood but this very handy library applies the &lt;a href="http://pandas.pydata.org/"&gt;pandas&lt;/a&gt; concept of labeled dimension to large N-dimension arrays prevalent in scientific computing. The result is an ease of manipulation of dimensions without having to guess or remember what they correspond to. Moreover, xarray plays nicely with two other relatively new libraries:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://dask.pydata.org/en/latest/"&gt;dask&lt;/a&gt;, which enables out of core computation so that memory availability becomes much less an issue with large data sets; &lt;/li&gt;
&lt;li&gt;&lt;a href="http://geo.holoviews.org/"&gt;GeoViews&lt;/a&gt;, a library sitting on top of &lt;a href="http://holoviews.org/"&gt;HoloViews&lt;/a&gt;. The latter eases the burden of data visualization by offering an unusual approach that does away with step-by-step graphical coding and allows the user to concentrate how the data organization instead. This results in a substantial reduction code written, which makes data analysis much cleaner and less bug-prone. GeoViews sits on top of the visualization package HoloViews, with an emphasis on geophysical data. It's also my first good-bye to the aging (10+ years) matplotlib library. It'll still be handy now and then, but it's time to try new things. &lt;p&gt;&lt;a href="https://madhatter106.github.io/DataScienceCorner/posts/xarray-geoviews-a-new-perspective-on-oceanographic-data-part-i/"&gt;Read more…&lt;/a&gt; (5 min remaining to read)&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Jupyter notebook</category><category>ocean color</category><category>xarray</category><guid>https://madhatter106.github.io/DataScienceCorner/posts/xarray-geoviews-a-new-perspective-on-oceanographic-data-part-i/</guid><pubDate>Mon, 20 Feb 2017 16:16:55 GMT</pubDate></item><item><title>Titanic Diaries - Part I: Data Dusting</title><link>https://madhatter106.github.io/DataScienceCorner/posts/titanic-i-cleanup/</link><dc:creator>Erdem Karaköylü</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In most data science tutorials I have seen, a lot of the data clean-up is done in what seems to me casually, an annoying obstacle to get to the sexy Machine Learning bit. I was curious to see what if any difference it made in my Kaggle ranking if I used a somewhat more cautious approach in my data cleanup. My approach was to dumbly follow &lt;a href="https://www.datacamp.com/community/open-courses/kaggle-python-tutorial-on-machine-learning#gs.EkI78Vw"&gt;Datacamp's tutorial&lt;/a&gt; and submit my test set labels as a benchmark. The second step is then to use a more elaborate data cleanup process and see whether taking the extra time actually moves my ranking up, or maybe down.
&lt;/p&gt;&lt;p&gt;&lt;a href="https://madhatter106.github.io/DataScienceCorner/posts/titanic-i-cleanup/"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Ipython</category><category>Jupyter notebook</category><category>kaggle</category><category>titanic</category><guid>https://madhatter106.github.io/DataScienceCorner/posts/titanic-i-cleanup/</guid><pubDate>Mon, 09 Jan 2017 21:46:09 GMT</pubDate></item></channel></rss>