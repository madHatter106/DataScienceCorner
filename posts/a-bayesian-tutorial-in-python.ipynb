{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><u>Bayesian Modeling for the Busy and the Confused</u></center>\n",
    "\n",
    "Part of my day-to-day work, involves assessing predictive models used to infer geophysical variables from remote sensing data. The models,  training data, and the practices used suffer from the following issues, reducing the reliability of predictions: \n",
    "\n",
    "\n",
    "* Models: \n",
    "    1. Models are opaque, with implicit (read unstated) assumptions. This makes either understanding them or replicating their development challenging. \n",
    "    2. When available, the quantification of prediction uncertainty is the result of an ad-hoc approach based on unverified statistical assumptions.\n",
    "    3. Additional contextual information and/or domain knowledge cannot be included to constrain predictions.\n",
    "    4. When multiple models are available, the methodology for comparing/selecting/blending  models is lacking.<br><br>\n",
    "\n",
    "* Training Data:\n",
    "    1. Data is noisy collected using a variety of methods.\n",
    "    2. Data sets used for training global models are small, occasionally biased toward specific oceanographic biomes.\n",
    "    3. High risk of over-fitting.<br><br>\n",
    "    \n",
    "* Practices:\n",
    "    1. Over-emphasis on fitting training data:\n",
    "    2. Arbitrary outlier definition and removal;\n",
    "    3. Data hacking to improve model fit\n",
    "    4. High risk of over-fitting\n",
    "\n",
    "To address these concerns and attempt to overhaul model development in my group, I opted for  Bayesian modeling. While initially challenging to get into, after years of frequentist dogma. I quickly found this approach to be quite intuitive. To help my colleagues understand the approach I wrote a series of tutorial notebooks highlighting both general concepts and specific examples dealing with the prediction of biogeophysical quantities from satellite remote sensing data. The present notebook is the introductory tutorial, with the content displayed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='TOP'></a>\n",
    "\n",
    "## Notebook Contents\n",
    "\n",
    "\n",
    "1. [Basics: Joint probability, Inverse probability and Bayes' Theorem](#BASIC)\n",
    "2. [Example: Inferring the Statistical Distribution of Chlorophyll from Data](#JustCHL)\n",
    "    1. [Grid Approximation](#GRID)\n",
    "        1. [Impact of priors](#PriorImpact)\n",
    "        2. [Impact of data set size](#DataImpact)\n",
    "    2. [MCMC](#MCMC)\n",
    "    3. [PyMC3](#PyMC3)\n",
    "3. [Regression](#Reg)\n",
    "    1. [Data Preparation](#DataPrep)\n",
    "    2. [Regression in PyMC3](#RegPyMC3)\n",
    "    3. [Checking Priors](#PriorCheck)\n",
    "    4. [Model Fitting](#Mining)\n",
    "    5. [Flavors of Uncertainty](#UNC)\n",
    "4. [Conclusion](#Conclusion)\n",
    "5. [To be Continued...](#Next)\n",
    "\n",
    "\n",
    "Get the full notebook [here]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.display import Image, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, uniform\n",
    "import pymc3 as pm\n",
    "from theano import shared\n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as pl\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import ticker as mtick\n",
    "import cmocean.cm as cmo\n",
    "import arviz as ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "print('Versions:')\n",
    "print('---------')\n",
    "print(f'python:  {sys.version.split(\"|\")[0]}')\n",
    "print(f'numpy:   {np.__version__}')\n",
    "print(f'pandas:  {pd.__version__}')\n",
    "print(f'seaborn: {sb.__version__}')\n",
    "print(f'pymc3:   {pm.__version__}')\n",
    "print(f'arviz:   {ar.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore',  category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a id='BASIC'></a>\n",
    "[Back to Contents](#TOP)\n",
    "\n",
    "### 1. <u>Basics</u>:\n",
    "\n",
    "#### $\\Rightarrow$Joint probability, Inverse probability and Bayes'  rule\n",
    "<br>\n",
    "Here's a circumspect list of basic concepts that will help understand what is going on:\n",
    "\n",
    "* Joint probability of two events $A$, $B$:\n",
    "$$P(A, B)=P(A|B)\\times P(B)=P(B|A)\\times P(A)$$\n",
    "\n",
    "* If A and B are independent: $$P(A|B) = P(A)\\ \\leftrightarrow P(A,B) = P(A)\\times P(B)$$\n",
    "\n",
    "* Inverse probability:$$\\boxed{P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}}$$ \n",
    "\n",
    "$\\rightarrow$Inverse probability is handy when $P(A|B)$ is desired but hard to compute, but its counterpart, $P(B|A)$ is easy to compute. The result above which is derived directly from the joint probability formulation above, is referred to as Bayes' theorem/rule. One might ask next, how this is used to build a \"Bayesian model.\"\n",
    "\n",
    "#### $\\Rightarrow$Extending Bayes' theorem to model building\n",
    "<br>\n",
    "Given a model:\n",
    "\n",
    "* Hypotheses (\\\\(H\\\\)): values that model parameters can take\n",
    "* \\\\( P(H) \\\\): probability of each value in H\n",
    "* Data (\\\\( D \\\\))\n",
    "* \\\\( P(D) \\\\): probability of the data, commonly referred to as \"Evidence.\"\n",
    "\n",
    "Approach \n",
    "* formulate initial opinion on what $H$ might include and with what probability, $P(H)$\n",
    "* collect data ($D$) \n",
    "* update $P(H)$ using $D$ and Bayes' theorem\n",
    "\n",
    "$$\\frac{P(H)\\times P(D|H)}{P(D)} = P(H|D)$$\n",
    "\n",
    "Computing the \"Evidence\", P(D), can yield intractable integrals to solve. Fortunately, it turns out that we can approximate the posterior, and give those integrals a wide berth. Hereafter, P(D), will be considered a normalization constant and will therefore be dropped; without prejudice, as it turns out.\n",
    "$$\\boxed{P(H) \\times P(D|H) \\propto P(H|D)}$$\n",
    "\n",
    "\n",
    "Note that what we care about is updating H, model parameters, after evaluating some observations.\n",
    "Let's go over each of the elements of this proportionality statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The prior\n",
    "$$\\underline{P(H)}\\times P(D|H) \\propto P(H|D)$$\n",
    "\n",
    "* $H$: set of values that model parameters might take with corresponding probability $P(H)$.\n",
    "* Priors should encompass justifiable assumptions/context information and nothing more.\n",
    "* We can use probability distributions to express $P(H)$ as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# build 1D grid of possible values\n",
    "h = np.linspace(-5, 5, num=100)\n",
    "μ_prior = 1.4 \n",
    "σ_prior = 0.86\n",
    "# compute probability at each a h\n",
    "p_h = norm.pdf(h, loc=μ_prior, scale=σ_prior) \n",
    "pl.plot(h, p_h); pl.ylabel('P(H)'); pl.xlabel('H');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The likelihood\n",
    "\n",
    "$$P(H)\\times \\underline{P(D|H)} \\propto  P(H|D)$$\n",
    "\n",
    "\n",
    "* probability of the data, \\\\(D\\\\), *given* \\\\(H\\\\).\n",
    "* in the frequentist framework, this quantity is maximized to find the \"best\" fit \\\\(\\rightarrow\\\\) Likelihood Maximization.\n",
    "    * maximizing the likelihood means finding a particular value for H, \\\\(\\hat{H}\\\\).\n",
    "    * for simple models and uninformative priors, \\\\(\\hat{H}\\\\) often corresponds to the mode of the Bayesian posterior (see below).\n",
    "    * likelihood maximization discards a lot of potentially valuable information (the posterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### The posterior:\n",
    "\n",
    "$$P(H)\\times P(D|H) \\propto \\underline{P(H|D)}$$\n",
    "\n",
    "\n",
    "* it's what Bayesians are after!!!\n",
    "* updated probability of \\\\(H\\\\) after exposing the model to \\\\(D\\\\).\n",
    "* used as prior for next iteration \\\\(P(H|D)\\rightarrow P(H)\\\\), when new data become available.\n",
    "* $P(H|D)$ naturally yields uncertainty around the estimate via propagation.\n",
    "\n",
    "\n",
    "In the next section I will attempt to illustrate the mechanics of Bayesian inference on real-world data.\n",
    "\n",
    "[Back to Contents](#TOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='JustCHL'></a>\n",
    "## Bayesian \"Hello World\": Inferring the Statistical Distribution of Chlorophyll\n",
    "\n",
    "The goal of Bayesian modeling is to approximate the process generating the data that is at hand, or that will be collected in the future. The model usually has a deterministic form that to relates inputs (predictors) to outputs (targets). In a first instance predictors are ignored and realism is sacrificed in favor of building intuition. \n",
    "\n",
    "Assuming that the process that generates the chlorophyll measurements observed can be approximated, <u>after log-transformation of the data</u>, by a dsGaussian distribution, with some scalar parameters; namely a constant central tendency, \\\\(\\mu\\\\), an a constant spread \\\\(\\sigma\\\\).  In other words, this distribution is not expected to vary as a result of any forcing. \n",
    "\n",
    "I will contrast two major approaches. Grid approximation, and Markov Chain Monte-Carlo. These are both approximations because, as mentioned earlier, the evidence \\\\(P(D)\\\\) is ignored. Instead, in both methods, relative probabilities are computed and subsequently normalized so as to add to 1.\n",
    "\n",
    "### Grid Approximation\n",
    "\n",
    "Grid approximation is so-called because the construction of such a grid describes all the possible combinations of parameter values to infer upon. Thus, each dimension of this grid corresponds to a model parameter and defines a range and a resolution. The resolution of the grid depends on the computational power available and the requirements of the problem at hand.I will illustrate that as the model complexity increases, along with the number of parameters featured, the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) can quickly take hold and limit the usefulness of this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='GRID'></a>\n",
    "#### Building the Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For this example I simply want to approximate the distribution of *chl_l* following these steps: \n",
    "\n",
    "* Define a model to approximate the process that generates the observations\n",
    "    * Theory: data generation is well approximated by a Gaussian.\n",
    "    * Hypotheses (\\\\(H\\\\)) therefore include 2 vectors; mean \\\\(\\mu\\\\) and standard deviation \\\\(\\sigma\\\\).\n",
    "    * Both parameters are expected to vary within a certain range.\n",
    "* Build the grid of model parameters\n",
    "    * 2D grid of \\\\((\\mu, \\sigma)\\\\) pair\n",
    "* Propose priors\n",
    "    * define priors for both \\\\(\\mu\\\\) and \\\\(\\sigma\\\\)\n",
    "* Compute likelihood\n",
    "* Compute posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, I load data stored in a pandas dataframe that contains among other things, log-transformed phytoplankton chlorophyll (*chl_l*) values measured during oceanographic cruises around the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_pickle('./pickleJar/df_logMxBlues.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_data[['MxBl-Gr', 'chl_l']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here are two columns. *MxBl-Gr* is a blue-to-green ratio that will serve as predictor of chlorophyll when I address regression. For now, *MxBl-Gr* is ignored, only *chl_l* is of interest. Here is what the distribution of *chl_l*, smoothed by kernel density estimation, looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots(figsize=(4,4))\n",
    "sb.kdeplot(df_data.chl_l, ax=ax, legend=False);\n",
    "ax.set_xlabel('chl_l');\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/fig1_chl.svg', dpi=300, format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Grid approximation needs a grid so I'll build a \\\\(200x200\\\\) grid with axes \\\\(\\mu\\\\) (mean) and \\\\(\\sigma\\\\) (std. dev.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "μ = np.linspace(-2, 2, num=200) # μ-axis\n",
    "σ = np.linspace(0, 2, num=200) # σ-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of manipulation I will use a pandas DataFrame to hold the results of the prior definition, likelihood and posterior computation at each grid point. Here is the definition of the grid..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_grid = pd.DataFrame([[μ_i, σ_i]\n",
    "                        for σ_i in σ for μ_i in μ], columns=['μ', 'σ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and here is what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(df_grid.shape)\n",
    "df_grid.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now I can define prior probabilities for both \\\\(\\mu\\\\) and \\\\(\\sigma\\\\). I will use a normal distribution for \\\\(\\mu\\\\) and a uniform distribution for \\\\(\\sigma\\\\) <br>\n",
    "\\\\(\\rightarrow \\mu \\sim \\mathcal{N}(1, 1)\\\\): a gaussian distribution centered at 1, with an sd of 1<br>\n",
    "\\\\(\\rightarrow \\sigma \\sim \\mathcal{U}(0, 2)\\\\): a uniform distribution bounded at 0 and 2<br>\n",
    "Note that limits are enforced to keep the computation manageable and the results illustrative but could be, in a real world situation, choices guided by domain knowledge. The lines below show how to pass the grid defined above to the scipy.stats distribution functions to compute the prior at each grid point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "μ_prior = norm.logpdf(df_grid.μ, 1, 1)\n",
    "σ_prior = uniform.logpdf(df_grid.σ, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that the code above computes the log of the prior probability of each parameter at each grid point. The parameters \\\\(\\mu\\\\) and \\\\(\\sigma\\\\) are assumed independent. Thus, according to the rules stipulated further above, the overall, i.e. joint prior probability at each grid point is just the product the individual prior probability at each grid point. Log-transforming probabilities means the joint probability of the entire grid can be computed by summing log probabilities followed by taking the exponent of the result. Doing so avoids problems like underflow.\n",
    "I store both the joint log-probability and the log-probability at each grid point in the pandas dataframe with the code snippet below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# log prior probability\n",
    "df_grid['log_prior_prob'] = μ_prior + σ_prior\n",
    "# straight prior probability\n",
    "df_grid['prior_prob'] = np.exp(df_grid.log_prior_prob\n",
    "                                - df_grid.log_prior_prob.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only two parameters, visualizing the joint prior probability is straighforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_grid.plot.hexbin(x='μ', y='σ', C='prior_prob', figsize=(7,6),\n",
    "                     cmap='plasma', sharex=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the figure above looking down the \\\\(\\sigma\\\\)-axis shows the 'wall' of uniform probability where none of the positive values, capped here at 2.0 has is expected to be more likely. Looking down the \\\\(\\mu\\\\)-axis, on the other hand, reveals the gaussian peak around 1, within a grid of floats extending from -2.0 to 2.0. \n",
    "Once priors have been defined, the model is ready to be fed some data. The *chl_* loaded earlier had several thousand observations. Because grid approximation is computationally intensive, I'll only pick a handful of data. For reasons discussed further below, this will enable the comparison of the effects different priors can have on the final result.\n",
    "I'll start by selecting 10 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample_N = 10\n",
    "df_data_s = df_data.dropna().sample(n=sample_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "g = sb.PairGrid(df_data_s.loc[:,['MxBl-Gr', 'chl_l']],\n",
    "                diag_sharey=False)\n",
    "g.map_diag(sb.kdeplot, )\n",
    "g.map_offdiag(sb.scatterplot, alpha=0.75, edgecolor='k');\n",
    "make_lower_triangle(g)\n",
    "g.axes[1,0].set_ylabel(r'$log_{10}(chl)$');\n",
    "g.axes[1,1].set_xlabel(r'$log_{10}(chl)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compute Log-Likelihood of the data given every  pair \\\\( ( \\mu ,\\sigma)\\\\). This is done by summing the log-probability of each datapoint, given each grid point; i.e. each \\\\((\\mu, \\sigma)\\\\) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_grid['LL'] = np.sum(norm.logpdf(df_data_s.chl_l.values.reshape(1, -1),\n",
    "                                    loc=df_grid.μ.values.reshape(-1, 1),\n",
    "                                    scale=df_grid.σ.values.reshape(-1, 1)\n",
    "                                   ), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Compute Posterior $P(\\mu,\\sigma\\ | data) \\propto P(data | \\mu, \\sigma) \\times P(\\mu, \\sigma)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# compute log-probability\n",
    "df_grid['log_post_prob'] = df_grid.LL + df_grid.log_prior_prob\n",
    "# convert to straight prob.\n",
    "df_grid['post_prob'] = np.exp(df_grid.log_post_prob\n",
    "                               - df_grid.log_post_prob.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Multi-Dimensional Prior and Posterior\n",
    "f, ax = pl.subplots(ncols=2, figsize=(12, 5), sharey=True)\n",
    "df_grid.plot.hexbin(x='μ', y='σ', C='prior_prob',\n",
    "                    cmap='plasma', sharex=False, ax=ax[0])\n",
    "df_grid.plot.hexbin(x='μ', y='σ', C='post_prob',\n",
    "                     cmap='plasma', sharex=False, ax=ax[1]);\n",
    "ax[0].set_title('Prior Probability Distribution')\n",
    "ax[1].set_title('Posterior Probability Distribution')\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/grid1.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/grid1.svg'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Compute Marginal Priors and Posteriors for each Parameter\n",
    "df_μ = df_grid.groupby(['μ']).sum().drop('σ', axis=1)[['prior_prob',\n",
    "                                                        'post_prob']\n",
    "                                                      ].reset_index()\n",
    "df_σ = df_grid.groupby(['σ']).sum().drop('μ', axis=1)[['prior_prob',\n",
    "                                                        'post_prob']\n",
    "                                                      ].reset_index() \n",
    "\n",
    "# Normalize Probability Distributions\n",
    "df_μ.prior_prob /= df_μ.prior_prob.max()\n",
    "df_μ.post_prob /= df_μ.post_prob.max()\n",
    "df_σ.prior_prob /= df_σ.prior_prob.max()\n",
    "df_σ.post_prob /= df_σ.post_prob.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Plot Marginal Priors and Posteriors\n",
    "f, ax = pl.subplots(ncols=2, figsize=(12, 4))\n",
    "df_μ.plot(x='μ', y='prior_prob', ax=ax[0], label='prior');\n",
    "df_μ.plot(x='μ', y='post_prob', ax=ax[0], label='posterior')\n",
    "df_σ.plot(x='σ', y='prior_prob', ax=ax[1], label='prior')\n",
    "df_σ.plot(x='σ', y='post_prob', ax=ax[1], label='posterior');\n",
    "f.suptitle('Marginal Probability Distributions', fontsize=16);\n",
    "f.tight_layout(pad=2)\n",
    "f.savefig('./figJar/Presentation/grid2.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='PriorImpact'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Impact of Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def compute_bayes_framework(data, priors_dict):\n",
    "    # build grid:\n",
    "    μ = np.linspace(-2, 2, num=200)\n",
    "    σ = np.linspace(0, 2, num=200)\n",
    "    df_b = pd.DataFrame([[μ_i, σ_i] for σ_i in σ for μ_i in μ],\n",
    "                        columns=['μ', 'σ'])\n",
    "    # compute/store distributions\n",
    "    μ_prior = norm.logpdf(df_b.μ, priors_dict['μ_mean'],\n",
    "                          priors_dict['μ_sd'])\n",
    "    σ_prior = uniform.logpdf(df_b.σ, priors_dict['σ_lo'],\n",
    "                             priors_dict['σ_hi'])\n",
    "    # compute joint prior\n",
    "    df_b['log_prior_prob'] = μ_prior + σ_prior \n",
    "    df_b['prior_prob'] = np.exp(df_b.log_prior_prob\n",
    "                                    - df_b.log_prior_prob.max())\n",
    "    # compute log likelihood\n",
    "    df_b['LL'] = np.sum(norm.logpdf(data.chl_l.values.reshape(1, -1),\n",
    "                                    loc=df_b.μ.values.reshape(-1, 1),\n",
    "                                    scale=df_b.σ.values.reshape(-1, 1)\n",
    "                                   ), axis=1)\n",
    "    # compute joint posterior\n",
    "    df_b['log_post_prob'] = df_b.LL + df_b.log_prior_prob\n",
    "    df_b['post_prob'] = np.exp(df_b.log_post_prob\n",
    "                               - df_b.log_post_prob.max())\n",
    "    return df_b\n",
    "\n",
    "\n",
    "def plot_posterior(df_, ax1, ax2):\n",
    "    df_.plot.hexbin(x='μ', y='σ', C='prior_prob',\n",
    "                        cmap='plasma', sharex=False, ax=ax1)\n",
    "    df_.plot.hexbin(x='μ', y='σ', C='post_prob',\n",
    "                         cmap='plasma', sharex=False, ax=ax2);\n",
    "    ax1.set_title('Prior Probability Distribution')\n",
    "    ax2.set_title('Posterior Probability Distribution')\n",
    "    \n",
    "    \n",
    "def plot_marginals(df_, ax1, ax2, plot_prior=True):\n",
    "    \"\"\"Compute marginal posterior distributions.\"\"\"\n",
    "    df_μ = df_.groupby(['μ']).sum().drop('σ',\n",
    "                                         axis=1)[['prior_prob',\n",
    "                                                  'post_prob']\n",
    "                                                ].reset_index()\n",
    "    df_σ = df_.groupby(['σ']).sum().drop('μ',\n",
    "                                         axis=1)[['prior_prob',\n",
    "                                                  'post_prob']\n",
    "                                                ].reset_index() \n",
    "    \n",
    "    # Normalize Probability Distributions\n",
    "    df_μ.prior_prob /= df_μ.prior_prob.max()\n",
    "    df_μ.post_prob /= df_μ.post_prob.max()\n",
    "    df_σ.prior_prob /= df_σ.prior_prob.max()\n",
    "    df_σ.post_prob /= df_σ.post_prob.max()\n",
    "\n",
    "    #Plot Marginal Priors and Posteriors\n",
    "    if plot_prior:\n",
    "        df_μ.plot(x='μ', y='prior_prob', ax=ax1, label='prior');\n",
    "        df_σ.plot(x='σ', y='prior_prob', ax=ax2, label='prior')\n",
    "    df_μ.plot(x='μ', y='post_prob', ax=ax1, label='posterior')\n",
    "    df_σ.plot(x='σ', y='post_prob', ax=ax2, label='posterior');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Try two priors:\n",
    "1. $\\mu \\sim \\mathcal{N}(1, 1)$, $\\sigma \\sim \\mathcal{U}(0, 2)$ - a weakly informative set of priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "weak_prior=dict(μ_mean=1, μ_sd=1, σ_lo=0, σ_hi=2)\n",
    "df_grid_1 = compute_bayes_framework(df_data_s, priors_dict=weak_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f , axp = pl.subplots(ncols=2, nrows=2, figsize=(12, 9))\n",
    "axp = axp.ravel()\n",
    "plot_posterior(df_grid_1, axp[0], axp[1])\n",
    "plot_marginals(df_grid_1, axp[2], axp[3])\n",
    "axp[2].legend(['weak prior', 'posterior'])\n",
    "axp[3].legend(['flat prior', 'posterior'])\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/grid3.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"./resources/grid3.svg?modified=3\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. $\\mu \\sim \\mathcal{N}(-1.5, 0.1)$, $\\sigma \\sim \\mathcal{U}(0, 2)$ - a strongly informative prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "strong_prior=dict(μ_mean=-1.5, μ_sd=.1, σ_lo=0, σ_hi=2)\n",
    "df_grid_2 = compute_bayes_framework(df_data_s, priors_dict=strong_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f , axp = pl.subplots(ncols=2, nrows=2, figsize=(12, 9))\n",
    "axp = axp.ravel()\n",
    "plot_posterior(df_grid_2, axp[0], axp[1])\n",
    "plot_marginals(df_grid_2, axp[2], axp[3])\n",
    "axp[2].legend(['strong prior', 'posterior'])\n",
    "axp[3].legend(['flat prior', 'posterior'])\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/grid4.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='DataImpact'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Impact of data set size\n",
    "* sub-sample size is now 500 samples,\n",
    "* same two priors used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample_N = 500\n",
    "\n",
    "# compute the inference dataframe\n",
    "df_data_s = df_data.dropna().sample(n=sample_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# display the new sub-sample\n",
    "g = sb.PairGrid(df_data_s.loc[:,['MxBl-Gr', 'chl_l']],\n",
    "                diag_sharey=False)\n",
    "g.map_diag(sb.kdeplot, )\n",
    "g.map_offdiag(sb.scatterplot, alpha=0.75, edgecolor='k');\n",
    "make_lower_triangle(g)\n",
    "g.axes[1,0].set_ylabel(r'$log_{10}(chl)$');\n",
    "g.axes[1,1].set_xlabel(r'$log_{10}(chl)$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_grid_3 = compute_bayes_framework(df_data_s, priors_dict=weak_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f , axp = pl.subplots(ncols=2, nrows=2, figsize=(12, 9))\n",
    "axp = axp.ravel()\n",
    "plot_posterior(df_grid_3, axp[0], axp[1])\n",
    "plot_marginals(df_grid_3, axp[2], axp[3])\n",
    "axp[2].legend(['weak prior', 'posterior'])\n",
    "axp[3].legend(['flat prior', 'posterior'])\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/grid5.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./resources/grid5.svg/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_grid_4 = compute_bayes_framework(df_data_s, priors_dict=strong_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f , axp = pl.subplots(ncols=2, nrows=2, figsize=(12, 9))\n",
    "axp = axp.ravel()\n",
    "plot_posterior(df_grid_4, axp[0], axp[1])\n",
    "plot_marginals(df_grid_4, axp[2], axp[3])\n",
    "axp[2].legend(['strong prior', 'posterior'])\n",
    "axp[3].legend(['flat prior', 'posterior'])\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/grid6.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./resources/grid6.svg/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f , axp = pl.subplots(ncols=2, nrows=2, figsize=(12, 8), sharey=True)\n",
    "axp = axp.ravel()\n",
    "plot_marginals(df_grid_3, axp[0], axp[1])\n",
    "plot_marginals(df_grid_4, axp[2], axp[3])\n",
    "axp[0].legend(['weak prior', 'posterior'])\n",
    "axp[1].legend(['flat prior', 'posterior'])\n",
    "axp[2].legend(['strong prior', 'posterior'])\n",
    "axp[3].legend(['flat prior', 'posterior'])\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/grid7.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***And using all the data?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "priors=dict(μ_mean=-1.5, μ_sd=.1, σ_lo=0, σ_hi=2)\n",
    "try:\n",
    "    df_grid_all_data= compute_bayes_framework(df_data, priors_dict=priors)\n",
    "except MemoryError:\n",
    "    print(\"OUT OF MEMORY!\")\n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Under the hood: Inferring chlorophyll distribution\n",
    "\n",
    "* ~~Grid approximation: computing probability everywhere~~\n",
    "* <font color='red'>Magical MCMC: Dealing with computational complexity</font>\n",
    "* Probabilistic Programming with PyMC3: Industrial grade MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id=\"MCMC\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Magical MCMC: Dealing with computational complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Grid approximation:\n",
    "    * useful for understanding mechanics of Bayesian computation\n",
    "    * computationally intensive\n",
    "    * impractical and often intractable for large data sets or high-dimension models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    \n",
    "* MCMC allows sampling <u>where it **probabilistically matters**</u>:\n",
    "    * compute current probability given location in parameter space\n",
    "    * propose jump to new location in parameter space\n",
    "    * compute new probability at proposed location\n",
    "    * jump to new location if $\\frac{new\\ probability}{current\\ probability}>1$ \n",
    "    * jump to new location if $\\frac{new\\ probability}{current\\ probability}>\\gamma\\in [0, 1]$\n",
    "    * otherwise stay in current location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mcmc(data, μ_0=0.5, n_samples=1000,):\n",
    "    print(f'{data.size} data points')\n",
    "    data = data.reshape(1, -1)\n",
    "    # set priors\n",
    "    σ=0.75 # keep σ fixed for simplicity\n",
    "    trace_μ = np.nan * np.ones(n_samples) # trace: where the sampler has been\n",
    "    trace_μ[0] = μ_0 # start with a first guess\n",
    "    for i in range(1, n_samples):\n",
    "        proposed_μ = norm.rvs(loc=trace_μ[i-1], scale=0.1, size=1)\n",
    "        prop_par_dict = dict(μ=proposed_μ, σ=σ)\n",
    "        curr_par_dict = dict(μ=trace_μ[i-1], σ=σ)\n",
    "        log_prob_prop = get_log_lik(data, prop_par_dict\n",
    "                                   ) + get_log_prior(prop_par_dict)\n",
    "        log_prob_curr = get_log_lik(data, curr_par_dict\n",
    "                                   ) + get_log_prior(curr_par_dict) \n",
    "        ratio = np.exp(log_prob_prop -  log_prob_curr)\n",
    "        if ratio > 1:\n",
    "            # accept proposal\n",
    "            trace_μ[i] = proposed_μ\n",
    "        else:\n",
    "            # evaluate low proba proposal\n",
    "            if uniform.rvs(size=1, loc=0, scale=1) > ratio:\n",
    "                # reject proposal\n",
    "                trace_μ[i] = trace_μ[i-1]    \n",
    "            else:\n",
    "                # accept proposal\n",
    "                trace_μ[i] = proposed_μ\n",
    "    return trace_μ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    " def get_log_lik(data, param_dict):\n",
    "    return np.sum(norm.logpdf(data, loc=param_dict['μ'],\n",
    "                              scale=param_dict['σ']\n",
    "                             ),\n",
    "                  axis=1)\n",
    "\n",
    "def get_log_prior(par_dict, loc=1, scale=1):\n",
    "    return norm.logpdf(par_dict['μ'], loc=loc, scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Timing MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mcmc_n_samples = 2000\n",
    "trace1 = mcmc(data=df_data_s.chl_l.values, n_samples=mcmc_n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots(nrows=2, figsize=(8, 8))\n",
    "ax[0].plot(np.arange(mcmc_n_samples), trace1, marker='.',\n",
    "           ls=':', color='k')\n",
    "ax[0].set_title('trace of μ, 500 data points')\n",
    "ax[1].set_title('μ marginal posterior')\n",
    "pm.plots.kdeplot(trace1, ax=ax[1], label='mcmc',\n",
    "                 color='orange', lw=2, zorder=1)\n",
    "ax[1].legend(loc='upper left')\n",
    "ax[1].set_ylim(bottom=0)\n",
    "df_μ = df_grid_3.groupby(['μ']).sum().drop('σ',\n",
    "                                     axis=1)[['post_prob']\n",
    "                                            ].reset_index()\n",
    "ax2 = ax[1].twinx()\n",
    "df_μ.plot(x='μ', y='post_prob', ax=ax2, color='k',\n",
    "         label='grid',)\n",
    "ax2.set_ylim(bottom=0);\n",
    "ax2.legend(loc='upper right')\n",
    "f.tight_layout()\n",
    "\n",
    "f.savefig('./figJar/Presentation/mcmc_1.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./resources/mcmc_1.svg?modified=\"1\"'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "samples = 2000\n",
    "trace2 = mcmc(data=df_data.chl_l.values, n_samples=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots(nrows=2, figsize=(8, 8))\n",
    "ax[0].plot(np.arange(samples), trace2, marker='.',\n",
    "           ls=':', color='k')\n",
    "ax[0].set_title(f'trace of μ, {df_data.chl_l.size} data points')\n",
    "ax[1].set_title('μ marginal posterior')\n",
    "pm.plots.kdeplot(trace2, ax=ax[1], label='mcmc',\n",
    "                 color='orange', lw=2, zorder=1)\n",
    "ax[1].legend(loc='upper left')\n",
    "ax[1].set_ylim(bottom=0)\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/mcmc_2.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./figJar/Presentation/mcmc_2.svg?modified=2'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots(ncols=2, figsize=(12, 5))\n",
    "ax[0].stem(pm.autocorr(trace1[1500:]))\n",
    "ax[1].stem(pm.autocorr(trace2[1500:]))\n",
    "ax[0].set_title(f'{df_data_s.chl_l.size} data points')\n",
    "ax[1].set_title(f'{df_data.chl_l.size} data points')\n",
    "f.suptitle('trace autocorrelation', fontsize=19)\n",
    "f.savefig('./figJar/Presentation/grid8.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots(nrows=2, figsize=(8, 8))\n",
    "thinned_trace = np.random.choice(trace2[100:], size=200, replace=False)\n",
    "ax[0].plot(np.arange(200), thinned_trace, marker='.',\n",
    "           ls=':', color='k')\n",
    "ax[0].set_title('thinned trace of μ')\n",
    "ax[1].set_title('μ marginal posterior')\n",
    "pm.plots.kdeplot(thinned_trace, ax=ax[1], label='mcmc',\n",
    "                 color='orange', lw=2, zorder=1)\n",
    "ax[1].legend(loc='upper left')\n",
    "ax[1].set_ylim(bottom=0)\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/grid9.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots()\n",
    "ax.stem(pm.autocorr(thinned_trace[:20]));\n",
    "f.savefig('./figJar/Presentation/stem2.svg', dpi=300, format='svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What's going on?\n",
    "\n",
    "Highly autocorrelated trace: <br>\n",
    "$\\rightarrow$ inadequate parameter space exploration<br>\n",
    "$\\rightarrow$ poor convergence..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Metropolis MCMC<br>\n",
    "    $\\rightarrow$ easy to implement + memory efficient<br>\n",
    "    $\\rightarrow$ inefficient parameter space exploration<br>\n",
    "    $\\rightarrow$ better MCMC sampler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Hamiltonian Monte Carlo (HMC)**\n",
    "* Greatly improved convergence\n",
    "* Well mixed traces are a signature and an easy diagnostic\n",
    "* HMC does require a lot of tuning,\n",
    "* Not practical for the inexperienced applied statistician or scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* No-U-Turn Sampler (NUTS), HMC that automates most tuning steps\n",
    "* NUTS  scales well to complex problems with many parameters (1000's)\n",
    "* Implemented in popular libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Probabilistic modeling for the beginner\n",
    "* <font color='red'>Under the hood: Inferring chlorophyll distribution</font>\n",
    "    * ~~Grid approximation: computing probability everywhere~~\n",
    "    * ~~MCMC: how it works~~\n",
    "    * <font color='red'>Probabilistic Programming with PyMC3: Industrial grade MCMC </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='PyMC3'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <u>Probabilistic Programming with PyMC3</u>\n",
    "\n",
    "* relatively simple syntax\n",
    "* easily used in conjuction with mainstream python scientific data structures<br>\n",
    "    $\\rightarrow$numpy arrays <br>\n",
    "    $\\rightarrow$pandas dataframes\n",
    "* models of reasonable complexity span ~10-20 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m1:\n",
    "    μ_ = pm.Normal('μ', mu=1, sd=1)\n",
    "    σ = pm.Uniform('σ', lower=0, upper=2)\n",
    "    lkl = pm.Normal('likelihood', mu=μ_, sd=σ,\n",
    "                    observed=df_data.chl_l.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "graph_m1 = pm.model_to_graphviz(m1)\n",
    "graph_m1.format = 'svg'\n",
    "graph_m1.render('./figJar/Presentation/graph_m1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./resources/graph_m1.svg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with m1:\n",
    "    trace_m1 = pm.sample(2000, tune=1000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace_m1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ar.plot_posterior(trace_m1, kind='hist', round_to=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id='Reg'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <u><font color='purple'>Tutorial Overview:</font></u>\n",
    "* Probabilistic modeling for the beginner<br>\n",
    "    $\\rightarrow$~~The basics~~<br>\n",
    "    $\\rightarrow$~~Starting easy: inferring chlorophyll~~<br>\n",
    "    <font color='red'>$\\rightarrow$Regression: adding a predictor to estimate chlorophyll</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='DataPrep'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression: Adding a  predictor to estimate chlorophyll\n",
    "\n",
    "* <font color=red>Data preparation</font>\n",
    "* Writing a regression model in PyMC3\n",
    "* Are my priors making sense?\n",
    "* Model fitting\n",
    "* Flavors of uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Linear regression takes the form\n",
    "\n",
    "$$ y = \\alpha + \\beta x $$\n",
    "where \n",
    "        $$\\ \\ \\ \\ \\ y = log_{10}(chl)$$ and $$x = log_{10}\\left(\\frac{Gr}{MxBl}\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_data['Gr-MxBl'] = -1 * df_data['MxBl-Gr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regression coefficients easier to interpret with centered predictor:<br><br>\n",
    "$$x_c = x - \\bar{x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_data['Gr-MxBl_c'] = df_data['Gr-MxBl'] - df_data['Gr-MxBl'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_data[['Gr-MxBl_c', 'chl_l']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_c = df_data.dropna()['Gr-MxBl_c'].values\n",
    "y = df_data.dropna().chl_l.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y = \\alpha + \\beta x_c$$<br>\n",
    "$\\rightarrow \\alpha=y$ when $x=\\bar{x}$<br>\n",
    "$\\rightarrow \\beta=\\Delta y$ when $x$ increases by one unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "g3 = sb.PairGrid(df_data.loc[:, ['Gr-MxBl_c', 'chl_l']], height=3,\n",
    "                         diag_sharey=False,)\n",
    "g3.map_diag(sb.kdeplot, color='k')\n",
    "g3.map_offdiag(sb.scatterplot, color='k');\n",
    "make_lower_triangle(g3)\n",
    "f = pl.gcf()\n",
    "axs = f.get_axes()\n",
    "xlabel = r'$log_{10}\\left(\\frac{Rrs_{green}}{max(Rrs_{blue})}\\right), centered$'\n",
    "ylabel = r'$log_{10}(chl)$'\n",
    "axs[0].set_xlabel(xlabel)\n",
    "axs[2].set_xlabel(xlabel)\n",
    "axs[2].set_ylabel(ylabel)\n",
    "axs[3].set_xlabel(ylabel)\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/pairwise_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='RegPyMC3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression: Adding a  predictor to estimate chlorophyll\n",
    "\n",
    "* ~~Data preparation~~\n",
    "* <font color=red>Writing a regression model in PyMC3</font>\n",
    "* Are my priors making sense?\n",
    "* Model fitting\n",
    "* Flavors of uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m_vague_prior:\n",
    "    # priors\n",
    "    σ = pm.Uniform('σ', lower=0, upper=2)\n",
    "    α = pm.Normal('α', mu=0, sd=1)\n",
    "    β = pm.Normal('β', mu=0, sd=1)\n",
    "    # deterministic model\n",
    "    μ = α + β * x_c\n",
    "    # likelihood\n",
    "    chl_i = pm.Normal('chl_i', mu=μ, sd=σ, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./resources/m_vague_graph.svg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='PriorCheck'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression: Adding a  predictor to estimate chlorophyll\n",
    "* ~~Data preparation~~\n",
    "* ~~Writing a regression model in PyMC3~~\n",
    "* <font color=red>Are my priors making sense?</font>\n",
    "* Model fitting \n",
    "* Flavors of uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vague_priors = pm.sample_prior_predictive(samples=500, model=m_vague_prior, vars=['α', 'β',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_dummy = np.linspace(-1.5, 1.5, num=50).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "α_prior_vague = vague_priors['α'].reshape(1, -1)\n",
    "β_prior_vague = vague_priors['β'].reshape(1, -1)\n",
    "chl_l_prior_μ_vague = α_prior_vague + β_prior_vague * x_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots( figsize=(6, 5))\n",
    "ax.plot(x_dummy, chl_l_prior_μ_vague, color='k', alpha=0.1,);\n",
    "ax.set_xlabel(r'$log_{10}\\left(\\frac{green}{max(blue)}\\right)$, centered')\n",
    "ax.set_ylabel('$log_{10}(chl)$')\n",
    "ax.set_title('Vague priors')\n",
    "ax.set_ylim(-3.5, 3.5)\n",
    "f.tight_layout(pad=1)\n",
    "f.savefig('./figJar/Presentation/prior_checks_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src='./figJar/Presentation/prior_checks_1.png?modified=3' width=65%>\n",
    "</center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m_informative_prior:\n",
    "    α = pm.Normal('α', mu=0, sd=0.2)\n",
    "    β = pm.Normal('β', mu=0, sd=0.5)\n",
    "    σ = pm.Uniform('σ', lower=0, upper=2)\n",
    "    μ = α + β * x_c\n",
    "    chl_i = pm.Normal('chl_i', mu=μ, sd=σ, observed=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "prior_info = pm.sample_prior_predictive(model=m_informative_prior, vars=['α', 'β'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "α_prior_info = prior_info['α'].reshape(1, -1)\n",
    "β_prior_info = prior_info['β'].reshape(1, -1)\n",
    "chl_l_prior_info = α_prior_info + β_prior_info * x_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots( figsize=(6, 5))\n",
    "ax.plot(x_dummy, chl_l_prior_info, color='k', alpha=0.1,);\n",
    "ax.set_xlabel(r'$log_{10}\\left(\\frac{green}{max(blue}\\right)$, centered')\n",
    "ax.set_ylabel('$log_{10}(chl)$')\n",
    "ax.set_title('Weakly informative priors')\n",
    "ax.set_ylim(-3.5, 3.5)\n",
    "f.tight_layout(pad=1)\n",
    "f.savefig('./figJar/Presentation/prior_checks_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='./resources/prior_checks_1.png?modif=1' />\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src='./resources/prior_checks_2.png?modif=2' />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='Mining'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression: Adding a  predictor to estimate chlorophyll\n",
    "* ~~Data preparatrion~~\n",
    "* ~~Writing a regression model in PyMC3~~\n",
    "* ~~Are my priors making sense?~~\n",
    "* <font color=red>Model fitting</font>\n",
    "* Flavors of uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with m_vague_prior:\n",
    "    trace_vague = pm.sample(2000, tune=1000, chains=4)\n",
    "\n",
    "with m_informative_prior:\n",
    "    trace_inf = pm.sample(2000, tune=1000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, axs = pl.subplots(ncols=2, nrows=2, figsize=(12, 7))\n",
    "ar.plot_posterior(trace_vague, var_names=['α', 'β'], round_to=2, ax=axs[0,:], kind='hist');\n",
    "ar.plot_posterior(trace_inf, var_names=['α', 'β'], round_to=2, ax=axs[1, :], kind='hist',\n",
    "                                        color='brown');\n",
    "axs[0,0].tick_params(rotation=20)\n",
    "axs[0,0].text(-0.137, 430, 'vague priors',\n",
    "              fontdict={'fontsize': 15})\n",
    "axs[1,0].tick_params(rotation=20)\n",
    "axs[1,0].text(-0.137, 430, 'informative priors',\n",
    "              fontdict={'fontsize': 15})\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/reg_posteriors.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src='./resources/reg_posteriors.svg'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='UNC'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression: Adding a  predictor to estimate chlorophyll\n",
    "* ~~Data preparation~~\n",
    "* ~~Writing a regression model in PyMC3~~\n",
    "* ~~Are my priors making sense?~~\n",
    "* ~~Data review and model fitting~~\n",
    "* <font color=red>Flavors of uncertainty</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two types of uncertainties:\n",
    "1. model uncertainty\n",
    "2. prediction uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "α_posterior = trace_inf.get_values('α').reshape(1, -1)\n",
    "β_posterior = trace_inf.get_values('β').reshape(1, -1)\n",
    "σ_posterior = trace_inf.get_values('σ').reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. model uncertainty: uncertainty around the model mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "μ_posterior = α_posterior + β_posterior * x_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pl.plot(x_dummy, μ_posterior[:, ::16], color='k', alpha=0.1);\n",
    "pl.plot(x_dummy, μ_posterior[:, 1], color='k', label='model mean')\n",
    "\n",
    "pl.scatter(x_c, y, color='orange', edgecolor='k', alpha=0.5, label='obs'); pl.legend();\n",
    "pl.ylim(-2.5, 2.5); pl.xlim(-1, 1);\n",
    "pl.xlabel(r'$log_{10}\\left(\\frac{Gr}{max(Blue)}\\right)$')\n",
    "pl.ylabel(r'$log_{10}(chlorophyll)$')\n",
    "f = pl.gcf()\n",
    "f.savefig('./figJar/Presentation/mu_posterior.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <img src='./resources/mu_posterior.svg/'>\n",
    "</center>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. prediction uncertainty: posterior predictive checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ppc = norm.rvs(loc=μ_posterior, scale=σ_posterior);\n",
    "ci_94_perc = pm.hpd(ppc.T, alpha=0.06);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pl.scatter(x_c, y, color='orange', edgecolor='k', alpha=0.5, label='obs'); pl.legend();\n",
    "pl.plot(x_dummy, ppc.mean(axis=1), color='k', label='mean prediction');\n",
    "pl.fill_between(x_dummy.flatten(), ci_94_perc[:, 0], ci_94_perc[:, 1], alpha=0.5, color='k',\n",
    "               label='94% credibility interval:\\n94% chance that prediction\\nwill be in here!');\n",
    "pl.xlim(-1, 1); pl.ylim(-2.5, 2.5)\n",
    "pl.legend(fontsize=12, loc='upper left')\n",
    "f = pl.gcf()\n",
    "f.savefig('./figJar/Presentation/ppc.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src='./resources/ppc.svg/' width=\"70%\"/>\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id=\"Conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In Conclusion Probabilistic Programming provides:\n",
    "* Transparent modeling:\n",
    "    * Explicit assumptions\n",
    "    * Easy to debate/criticize\n",
    "    * Easy to communicate/reproduce/improve upon\n",
    "* Posterior distribution much richer construct than point estimates\n",
    "* Principled estimation of model and prediction uncertainty\n",
    "* Accessibility\n",
    "    * Constantly improving algorithms\n",
    "    * Easy-to-use software\n",
    "    * Flexible framework, largely problem-agnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table><tr>\n",
    "    <td><img src='./resources/krusche_diagrams_hs_reg.png?modif=2'/></td>\n",
    "    <td><img src='./resources/krusche_diagrams_BNN.png?modif=1'/></td>\n",
    "    </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id=\"Next\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nikola": {
   "category": "",
   "date": "2019-07-21 10:27:18 UTC-04:00",
   "description": "",
   "link": "",
   "slug": "a-bayesian-tutorial-in-python",
   "tags": "",
   "title": "A Bayesian Tutorial in Python",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
