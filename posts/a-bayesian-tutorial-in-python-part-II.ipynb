{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><u><u>Bayesian Modeling for the Busy and the Confused - Part II</u></u></center>\n",
    "## <center><i>Markov Chain Monte-Carlo</i><center>\n",
    "Currently, the capacity to gather data is far ahead of the ability to generate meaningful insight using conventional approaches. Hopes of alleviating this bottleneck has come through the application of machine learning tools. Among these tools one that is increasingly garnering traction is probabilistic programming, particularly Bayesian modeling. In this paradigm, variables that are used to define models carry a probabilistic distribution rather than a scalar value. \"Fitting\" a model to data can then , simplistically, be construed as finding the appropriate parameterization for these distributions, given the model structure and the data. This offers a number of advantages over other methods, not the least of which is the estimation of uncertainty around model results. This in turn can better inform subsequent processes, such as decision-making, and/or scientific discovery.\n",
    "    \n",
    "<br><br>\n",
    "    \n",
    "The present is the first of a two-notebook series, the subject of which is a brief, basic, but hands-on programmatic introduction to Bayesian modeling. This notebook contains an  of a few key probability principles relevant to Bayesian inference.  An illustration of how to put these in practice follows. In particular, I will explain one of the conmore intuitve approaches to Bayesian computation; Grid Approximation (GA). With this framework I will show how to create simple models that can be used to interpret and predict real world data. <br>\n",
    "GA is computationally intensive and runs into problems quickly when the data set is large and/or the model increases in complexity. One of the more popular solutions to this problem is the use of the Markov Chain Monte-Carlo (MCMC) algorithm. The implementation of MCMC in Bayesian models will be the subject of the [second notebook of this series]().\n",
    "<br>\n",
    "As of this writing the most popular programming language in machine learning is Python. Python is an easy language to pickup: pedagogical resources abound. Python is free, open source, and a large number of very useful libraries have been written over the years that have propelled it to its current place of prominence in a number of fields, in addition to machine learning.\n",
    "<br><br>\n",
    "I use Python (3.6+) code to illustrate the mechanics of Bayesian inference in lieu of lengthy explanations. I also use a number of dedicated Python libraries that shortens the code considerably. A solid understanding of Bayesian modeling cannot be spoon-fed and can only come from getting one's hands dirty.. Emphasis is therefore on readable reproducible code. This should ease the work the interested has to do to get some practice re-running the notebook  and experimenting with some of the coding and Bayesian modeling patterns presented. Some know-how is required regarding installing and running a Python distribution, the required libraries, and jupyter notebooks; this is easily gleaned from the internet. A popular option in the machine learning community is [Anaconda](https://www.anaconda.com/distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='TOP'></a>\n",
    "\n",
    "## Notebook Contents\n",
    "\n",
    "\n",
    "1. [Basics: Joint probability, Inverse probability and Bayes' Theorem](#BASIC)\n",
    "2. [Example: Inferring the Statistical Distribution of Chlorophyll from Data](#JustCHL)\n",
    "    1. [Grid Approximation](#GRID)\n",
    "        1. [Impact of priors](#PriorImpact)\n",
    "        2. [Impact of data set size](#DataImpact)\n",
    "    2. [MCMC](#MCMC)\n",
    "    3. [PyMC3](#PyMC3)\n",
    "3. [Regression](#Reg)\n",
    "    1. [Data Preparation](#DataPrep)\n",
    "    2. [Regression in PyMC3](#RegPyMC3)\n",
    "    3. [Checking Priors](#PriorCheck)\n",
    "    4. [Model Fitting](#Mining)\n",
    "    5. [Flavors of Uncertainty](#UNC)\n",
    "4. [Final Comments](#Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.display import Image, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm as gaussian, uniform\n",
    "import pymc3 as pm\n",
    "from theano import shared\n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as pl\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import ticker as mtick\n",
    "import arviz as ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "print('Versions:')\n",
    "print('---------')\n",
    "print(f'python:  {sys.version.split(\"|\")[0]}')\n",
    "print(f'numpy:   {np.__version__}')\n",
    "print(f'pandas:  {pd.__version__}')\n",
    "print(f'seaborn: {sb.__version__}')\n",
    "print(f'pymc3:   {pm.__version__}')\n",
    "print(f'arviz:   {ar.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore',  category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Under the hood: Inferring chlorophyll distribution\n",
    "\n",
    "* ~~Grid approximation: computing probability everywhere~~\n",
    "* <font color='red'>Magical MCMC: Dealing with computational complexity</font>\n",
    "* Probabilistic Programming with PyMC3: Industrial grade MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id=\"MCMC\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Magical MCMC: Dealing with computational complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Grid approximation:\n",
    "    * useful for understanding mechanics of Bayesian computation\n",
    "    * computationally intensive\n",
    "    * impractical and often intractable for large data sets or high-dimension models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    \n",
    "* MCMC allows sampling <u>where it **probabilistically matters**</u>:\n",
    "    * compute current probability given location in parameter space\n",
    "    * propose jump to new location in parameter space\n",
    "    * compute new probability at proposed location\n",
    "    * jump to new location if $\\frac{new\\ probability}{current\\ probability}>1$ \n",
    "    * jump to new location if $\\frac{new\\ probability}{current\\ probability}>\\gamma\\in [0, 1]$\n",
    "    * otherwise stay in current location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mcmc(data, μ_0=0.5, n_samples=1000,):\n",
    "    print(f'{data.size} data points')\n",
    "    data = data.reshape(1, -1)\n",
    "    # set priors\n",
    "    σ=0.75 # keep σ fixed for simplicity\n",
    "    trace_μ = np.nan * np.ones(n_samples) # trace: where the sampler has been\n",
    "    trace_μ[0] = μ_0 # start with a first guess\n",
    "    for i in range(1, n_samples):\n",
    "        proposed_μ = norm.rvs(loc=trace_μ[i-1], scale=0.1, size=1)\n",
    "        prop_par_dict = dict(μ=proposed_μ, σ=σ)\n",
    "        curr_par_dict = dict(μ=trace_μ[i-1], σ=σ)\n",
    "        log_prob_prop = get_log_lik(data, prop_par_dict\n",
    "                                   ) + get_log_prior(prop_par_dict)\n",
    "        log_prob_curr = get_log_lik(data, curr_par_dict\n",
    "                                   ) + get_log_prior(curr_par_dict) \n",
    "        ratio = np.exp(log_prob_prop -  log_prob_curr)\n",
    "        if ratio > 1:\n",
    "            # accept proposal\n",
    "            trace_μ[i] = proposed_μ\n",
    "        else:\n",
    "            # evaluate low proba proposal\n",
    "            if uniform.rvs(size=1, loc=0, scale=1) > ratio:\n",
    "                # reject proposal\n",
    "                trace_μ[i] = trace_μ[i-1]    \n",
    "            else:\n",
    "                # accept proposal\n",
    "                trace_μ[i] = proposed_μ\n",
    "    return trace_μ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    " def get_log_lik(data, param_dict):\n",
    "    return np.sum(norm.logpdf(data, loc=param_dict['μ'],\n",
    "                              scale=param_dict['σ']\n",
    "                             ),\n",
    "                  axis=1)\n",
    "\n",
    "def get_log_prior(par_dict, loc=1, scale=1):\n",
    "    return norm.logpdf(par_dict['μ'], loc=loc, scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Timing MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mcmc_n_samples = 2000\n",
    "trace1 = mcmc(data=df_data_s.chl_l.values, n_samples=mcmc_n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots(nrows=2, figsize=(8, 8))\n",
    "ax[0].plot(np.arange(mcmc_n_samples), trace1, marker='.',\n",
    "           ls=':', color='k')\n",
    "ax[0].set_title('trace of μ, 500 data points')\n",
    "ax[1].set_title('μ marginal posterior')\n",
    "pm.plots.kdeplot(trace1, ax=ax[1], label='mcmc',\n",
    "                 color='orange', lw=2, zorder=1)\n",
    "ax[1].legend(loc='upper left')\n",
    "ax[1].set_ylim(bottom=0)\n",
    "df_μ = df_grid_3.groupby(['μ']).sum().drop('σ',\n",
    "                                     axis=1)[['post_prob']\n",
    "                                            ].reset_index()\n",
    "ax2 = ax[1].twinx()\n",
    "df_μ.plot(x='μ', y='post_prob', ax=ax2, color='k',\n",
    "         label='grid',)\n",
    "ax2.set_ylim(bottom=0);\n",
    "ax2.legend(loc='upper right')\n",
    "f.tight_layout()\n",
    "\n",
    "f.savefig('./figJar/Presentation/mcmc_1.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./resources/mcmc_1.svg?modified=\"1\"'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "samples = 2000\n",
    "trace2 = mcmc(data=df_data.chl_l.values, n_samples=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots(nrows=2, figsize=(8, 8))\n",
    "ax[0].plot(np.arange(samples), trace2, marker='.',\n",
    "           ls=':', color='k')\n",
    "ax[0].set_title(f'trace of μ, {df_data.chl_l.size} data points')\n",
    "ax[1].set_title('μ marginal posterior')\n",
    "pm.plots.kdeplot(trace2, ax=ax[1], label='mcmc',\n",
    "                 color='orange', lw=2, zorder=1)\n",
    "ax[1].legend(loc='upper left')\n",
    "ax[1].set_ylim(bottom=0)\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/mcmc_2.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./figJar/Presentation/mcmc_2.svg?modified=2'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots(ncols=2, figsize=(12, 5))\n",
    "ax[0].stem(pm.autocorr(trace1[1500:]))\n",
    "ax[1].stem(pm.autocorr(trace2[1500:]))\n",
    "ax[0].set_title(f'{df_data_s.chl_l.size} data points')\n",
    "ax[1].set_title(f'{df_data.chl_l.size} data points')\n",
    "f.suptitle('trace autocorrelation', fontsize=19)\n",
    "f.savefig('./figJar/Presentation/grid8.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots(nrows=2, figsize=(8, 8))\n",
    "thinned_trace = np.random.choice(trace2[100:], size=200, replace=False)\n",
    "ax[0].plot(np.arange(200), thinned_trace, marker='.',\n",
    "           ls=':', color='k')\n",
    "ax[0].set_title('thinned trace of μ')\n",
    "ax[1].set_title('μ marginal posterior')\n",
    "pm.plots.kdeplot(thinned_trace, ax=ax[1], label='mcmc',\n",
    "                 color='orange', lw=2, zorder=1)\n",
    "ax[1].legend(loc='upper left')\n",
    "ax[1].set_ylim(bottom=0)\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/grid9.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots()\n",
    "ax.stem(pm.autocorr(thinned_trace[:20]));\n",
    "f.savefig('./figJar/Presentation/stem2.svg', dpi=300, format='svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What's going on?\n",
    "\n",
    "Highly autocorrelated trace: <br>\n",
    "$\\rightarrow$ inadequate parameter space exploration<br>\n",
    "$\\rightarrow$ poor convergence..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Metropolis MCMC<br>\n",
    "    $\\rightarrow$ easy to implement + memory efficient<br>\n",
    "    $\\rightarrow$ inefficient parameter space exploration<br>\n",
    "    $\\rightarrow$ better MCMC sampler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Hamiltonian Monte Carlo (HMC)**\n",
    "* Greatly improved convergence\n",
    "* Well mixed traces are a signature and an easy diagnostic\n",
    "* HMC does require a lot of tuning,\n",
    "* Not practical for the inexperienced applied statistician or scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* No-U-Turn Sampler (NUTS), HMC that automates most tuning steps\n",
    "* NUTS  scales well to complex problems with many parameters (1000's)\n",
    "* Implemented in popular libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Probabilistic modeling for the beginner\n",
    "* <font color='red'>Under the hood: Inferring chlorophyll distribution</font>\n",
    "    * ~~Grid approximation: computing probability everywhere~~\n",
    "    * ~~MCMC: how it works~~\n",
    "    * <font color='red'>Probabilistic Programming with PyMC3: Industrial grade MCMC </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='PyMC3'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <u>Probabilistic Programming with PyMC3</u>\n",
    "\n",
    "* relatively simple syntax\n",
    "* easily used in conjuction with mainstream python scientific data structures<br>\n",
    "    $\\rightarrow$numpy arrays <br>\n",
    "    $\\rightarrow$pandas dataframes\n",
    "* models of reasonable complexity span ~10-20 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m1:\n",
    "    μ_ = pm.Normal('μ', mu=1, sd=1)\n",
    "    σ = pm.Uniform('σ', lower=0, upper=2)\n",
    "    lkl = pm.Normal('likelihood', mu=μ_, sd=σ,\n",
    "                    observed=df_data.chl_l.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "graph_m1 = pm.model_to_graphviz(m1)\n",
    "graph_m1.format = 'svg'\n",
    "graph_m1.render('./figJar/Presentation/graph_m1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./resources/graph_m1.svg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with m1:\n",
    "    trace_m1 = pm.sample(2000, tune=1000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace_m1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ar.plot_posterior(trace_m1, kind='hist', round_to=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id='Reg'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <u><font color='purple'>Tutorial Overview:</font></u>\n",
    "* Probabilistic modeling for the beginner<br>\n",
    "    $\\rightarrow$~~The basics~~<br>\n",
    "    $\\rightarrow$~~Starting easy: inferring chlorophyll~~<br>\n",
    "    <font color='red'>$\\rightarrow$Regression: adding a predictor to estimate chlorophyll</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='DataPrep'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression: Adding a  predictor to estimate chlorophyll\n",
    "\n",
    "* <font color=red>Data preparation</font>\n",
    "* Writing a regression model in PyMC3\n",
    "* Are my priors making sense?\n",
    "* Model fitting\n",
    "* Flavors of uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Linear regression takes the form\n",
    "\n",
    "$$ y = \\alpha + \\beta x $$\n",
    "where \n",
    "        $$\\ \\ \\ \\ \\ y = log_{10}(chl)$$ and $$x = log_{10}\\left(\\frac{Gr}{MxBl}\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_data['Gr-MxBl'] = -1 * df_data['MxBl-Gr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regression coefficients easier to interpret with centered predictor:<br><br>\n",
    "$$x_c = x - \\bar{x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_data['Gr-MxBl_c'] = df_data['Gr-MxBl'] - df_data['Gr-MxBl'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_data[['Gr-MxBl_c', 'chl_l']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_c = df_data.dropna()['Gr-MxBl_c'].values\n",
    "y = df_data.dropna().chl_l.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y = \\alpha + \\beta x_c$$<br>\n",
    "$\\rightarrow \\alpha=y$ when $x=\\bar{x}$<br>\n",
    "$\\rightarrow \\beta=\\Delta y$ when $x$ increases by one unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "g3 = sb.PairGrid(df_data.loc[:, ['Gr-MxBl_c', 'chl_l']], height=3,\n",
    "                         diag_sharey=False,)\n",
    "g3.map_diag(sb.kdeplot, color='k')\n",
    "g3.map_offdiag(sb.scatterplot, color='k');\n",
    "make_lower_triangle(g3)\n",
    "f = pl.gcf()\n",
    "axs = f.get_axes()\n",
    "xlabel = r'$log_{10}\\left(\\frac{Rrs_{green}}{max(Rrs_{blue})}\\right), centered$'\n",
    "ylabel = r'$log_{10}(chl)$'\n",
    "axs[0].set_xlabel(xlabel)\n",
    "axs[2].set_xlabel(xlabel)\n",
    "axs[2].set_ylabel(ylabel)\n",
    "axs[3].set_xlabel(ylabel)\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/pairwise_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='RegPyMC3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression: Adding a  predictor to estimate chlorophyll\n",
    "\n",
    "* ~~Data preparation~~\n",
    "* <font color=red>Writing a regression model in PyMC3</font>\n",
    "* Are my priors making sense?\n",
    "* Model fitting\n",
    "* Flavors of uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m_vague_prior:\n",
    "    # priors\n",
    "    σ = pm.Uniform('σ', lower=0, upper=2)\n",
    "    α = pm.Normal('α', mu=0, sd=1)\n",
    "    β = pm.Normal('β', mu=0, sd=1)\n",
    "    # deterministic model\n",
    "    μ = α + β * x_c\n",
    "    # likelihood\n",
    "    chl_i = pm.Normal('chl_i', mu=μ, sd=σ, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./resources/m_vague_graph.svg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='PriorCheck'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression: Adding a  predictor to estimate chlorophyll\n",
    "* ~~Data preparation~~\n",
    "* ~~Writing a regression model in PyMC3~~\n",
    "* <font color=red>Are my priors making sense?</font>\n",
    "* Model fitting \n",
    "* Flavors of uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vague_priors = pm.sample_prior_predictive(samples=500, model=m_vague_prior, vars=['α', 'β',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_dummy = np.linspace(-1.5, 1.5, num=50).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "α_prior_vague = vague_priors['α'].reshape(1, -1)\n",
    "β_prior_vague = vague_priors['β'].reshape(1, -1)\n",
    "chl_l_prior_μ_vague = α_prior_vague + β_prior_vague * x_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots( figsize=(6, 5))\n",
    "ax.plot(x_dummy, chl_l_prior_μ_vague, color='k', alpha=0.1,);\n",
    "ax.set_xlabel(r'$log_{10}\\left(\\frac{green}{max(blue)}\\right)$, centered')\n",
    "ax.set_ylabel('$log_{10}(chl)$')\n",
    "ax.set_title('Vague priors')\n",
    "ax.set_ylim(-3.5, 3.5)\n",
    "f.tight_layout(pad=1)\n",
    "f.savefig('./figJar/Presentation/prior_checks_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src='./figJar/Presentation/prior_checks_1.png?modified=3' width=65%>\n",
    "</center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m_informative_prior:\n",
    "    α = pm.Normal('α', mu=0, sd=0.2)\n",
    "    β = pm.Normal('β', mu=0, sd=0.5)\n",
    "    σ = pm.Uniform('σ', lower=0, upper=2)\n",
    "    μ = α + β * x_c\n",
    "    chl_i = pm.Normal('chl_i', mu=μ, sd=σ, observed=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "prior_info = pm.sample_prior_predictive(model=m_informative_prior, vars=['α', 'β'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "α_prior_info = prior_info['α'].reshape(1, -1)\n",
    "β_prior_info = prior_info['β'].reshape(1, -1)\n",
    "chl_l_prior_info = α_prior_info + β_prior_info * x_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = pl.subplots( figsize=(6, 5))\n",
    "ax.plot(x_dummy, chl_l_prior_info, color='k', alpha=0.1,);\n",
    "ax.set_xlabel(r'$log_{10}\\left(\\frac{green}{max(blue}\\right)$, centered')\n",
    "ax.set_ylabel('$log_{10}(chl)$')\n",
    "ax.set_title('Weakly informative priors')\n",
    "ax.set_ylim(-3.5, 3.5)\n",
    "f.tight_layout(pad=1)\n",
    "f.savefig('./figJar/Presentation/prior_checks_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='./resources/prior_checks_1.png?modif=1' />\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src='./resources/prior_checks_2.png?modif=2' />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='Mining'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression: Adding a  predictor to estimate chlorophyll\n",
    "* ~~Data preparatrion~~\n",
    "* ~~Writing a regression model in PyMC3~~\n",
    "* ~~Are my priors making sense?~~\n",
    "* <font color=red>Model fitting</font>\n",
    "* Flavors of uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with m_vague_prior:\n",
    "    trace_vague = pm.sample(2000, tune=1000, chains=4)\n",
    "\n",
    "with m_informative_prior:\n",
    "    trace_inf = pm.sample(2000, tune=1000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, axs = pl.subplots(ncols=2, nrows=2, figsize=(12, 7))\n",
    "ar.plot_posterior(trace_vague, var_names=['α', 'β'], round_to=2, ax=axs[0,:], kind='hist');\n",
    "ar.plot_posterior(trace_inf, var_names=['α', 'β'], round_to=2, ax=axs[1, :], kind='hist',\n",
    "                                        color='brown');\n",
    "axs[0,0].tick_params(rotation=20)\n",
    "axs[0,0].text(-0.137, 430, 'vague priors',\n",
    "              fontdict={'fontsize': 15})\n",
    "axs[1,0].tick_params(rotation=20)\n",
    "axs[1,0].text(-0.137, 430, 'informative priors',\n",
    "              fontdict={'fontsize': 15})\n",
    "f.tight_layout()\n",
    "f.savefig('./figJar/Presentation/reg_posteriors.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src='./resources/reg_posteriors.svg'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id='UNC'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression: Adding a  predictor to estimate chlorophyll\n",
    "* ~~Data preparation~~\n",
    "* ~~Writing a regression model in PyMC3~~\n",
    "* ~~Are my priors making sense?~~\n",
    "* ~~Data review and model fitting~~\n",
    "* <font color=red>Flavors of uncertainty</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two types of uncertainties:\n",
    "1. model uncertainty\n",
    "2. prediction uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "α_posterior = trace_inf.get_values('α').reshape(1, -1)\n",
    "β_posterior = trace_inf.get_values('β').reshape(1, -1)\n",
    "σ_posterior = trace_inf.get_values('σ').reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. model uncertainty: uncertainty around the model mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "μ_posterior = α_posterior + β_posterior * x_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pl.plot(x_dummy, μ_posterior[:, ::16], color='k', alpha=0.1);\n",
    "pl.plot(x_dummy, μ_posterior[:, 1], color='k', label='model mean')\n",
    "\n",
    "pl.scatter(x_c, y, color='orange', edgecolor='k', alpha=0.5, label='obs'); pl.legend();\n",
    "pl.ylim(-2.5, 2.5); pl.xlim(-1, 1);\n",
    "pl.xlabel(r'$log_{10}\\left(\\frac{Gr}{max(Blue)}\\right)$')\n",
    "pl.ylabel(r'$log_{10}(chlorophyll)$')\n",
    "f = pl.gcf()\n",
    "f.savefig('./figJar/Presentation/mu_posterior.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <img src='./resources/mu_posterior.svg/'>\n",
    "</center>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. prediction uncertainty: posterior predictive checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ppc = norm.rvs(loc=μ_posterior, scale=σ_posterior);\n",
    "ci_94_perc = pm.hpd(ppc.T, alpha=0.06);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pl.scatter(x_c, y, color='orange', edgecolor='k', alpha=0.5, label='obs'); pl.legend();\n",
    "pl.plot(x_dummy, ppc.mean(axis=1), color='k', label='mean prediction');\n",
    "pl.fill_between(x_dummy.flatten(), ci_94_perc[:, 0], ci_94_perc[:, 1], alpha=0.5, color='k',\n",
    "               label='94% credibility interval:\\n94% chance that prediction\\nwill be in here!');\n",
    "pl.xlim(-1, 1); pl.ylim(-2.5, 2.5)\n",
    "pl.legend(fontsize=12, loc='upper left')\n",
    "f = pl.gcf()\n",
    "f.savefig('./figJar/Presentation/ppc.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src='./resources/ppc.svg/' width=\"70%\"/>\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id=\"Conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In Conclusion Probabilistic Programming provides:\n",
    "* Transparent modeling:\n",
    "    * Explicit assumptions\n",
    "    * Easy to debate/criticize\n",
    "    * Easy to communicate/reproduce/improve upon\n",
    "* Posterior distribution much richer construct than point estimates\n",
    "* Principled estimation of model and prediction uncertainty\n",
    "* Accessibility\n",
    "    * Constantly improving algorithms\n",
    "    * Easy-to-use software\n",
    "    * Flexible framework, largely problem-agnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table><tr>\n",
    "    <td><img src='./resources/krusche_diagrams_hs_reg.png?modif=2'/></td>\n",
    "    <td><img src='./resources/krusche_diagrams_BNN.png?modif=1'/></td>\n",
    "    </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Back to Contents](#TOP)\n",
    "<a id=\"Next\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nikola": {
   "category": "",
   "date": "2019-07-21 10:27:18 UTC-04:00",
   "description": "",
   "link": "",
   "slug": "a-bayesian-tutorial-in-python",
   "tags": "",
   "title": "A Bayesian Tutorial in Python",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
